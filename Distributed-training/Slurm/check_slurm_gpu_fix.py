import os
import socket
import torch

# --- 1. è·å–åŸºç¡€èº«ä»½ä¿¡æ¯ ---
rank = int(os.environ.get("SLURM_PROCID", "0"))      # å…¨å±€ Rank (èº«ä»½è¯)
num_tasks = int(os.environ.get("SLURM_NTASKS", "1")) # æ€»äººæ•°
node_name = socket.gethostname()                     # æˆ¿é—´å·
pid = os.getpid()                                    # è¿›ç¨‹å·

# --- 2. è·å– GPU ç‰©ç†ä¿¡æ¯ (æ€»å…±æœ‰å‡ æŠŠæ¤…å­) ---
gpu_available = torch.cuda.is_available()
# ã€å…³é”®ä¿®æ­£ã€‘ç›´æ¥é—® PyTorch æœ‰å‡ å¼ å¡ï¼Œä¸ä¾èµ–ä¸ç¨³å®šçš„ç¯å¢ƒå˜é‡
gpus_per_node = torch.cuda.device_count() if gpu_available else 0

# --- 3. ã€æ ¸å¿ƒé€»è¾‘ã€‘è®¡ç®—å¹¶ç»‘å®šåº§ä½ ---
current_binding_info = "âŒ æœªç»‘å®š (CPUæ¨¡å¼)"
local_rank = 0

if gpu_available and gpus_per_node > 0:
    # A. è®¡ç®—ï¼šæˆ‘æ˜¯è¿™å°æœºå™¨ä¸Šçš„ç¬¬å‡ ä¸ªäººï¼Ÿ (Rank 0->0, Rank 1->1, Rank 4->0 ...)
    local_rank = rank % gpus_per_node
    
    # B. åŠ¨ä½œï¼šå¼ºåˆ¶åä¸‹ï¼(è¿™å°±æ˜¯â€œåˆ†æ¤…å­â€çš„åŠ¨ä½œ)
    torch.cuda.set_device(local_rank)
    
    # C. éªŒè¯ï¼šç°åœ¨ PyTorch è®¤ä¸ºæˆ‘å½“å‰çš„ä¸»è®¾å¤‡æ˜¯è°ï¼Ÿ
    current_device_idx = torch.cuda.current_device()
    current_device_name = torch.cuda.get_device_name(current_device_idx)
    
    # D. å®æµ‹ï¼šåˆ›å»ºä¸€ä¸ªå¼ é‡ï¼Œçœ‹å®ƒè‡ªåŠ¨è½åœ¨å“ªå¼ å¡ä¸Š
    test_tensor = torch.tensor([1]).cuda()
    
    current_binding_info = (
        f"âœ… å·²ç»‘å®šé€»è¾‘åº§ä½: {local_rank}\n"
        f"   -> éªŒè¯ current_device(): {current_device_idx}\n"
        f"   -> éªŒè¯ Tensor ä½ç½®: {test_tensor.device}\n"
        f"   -> ç¡¬ä»¶å‹å·: {current_device_name}"
    )

# --- 4. æ‰“å°ç»“æœ ---
info = f"""
========================================
ğŸ‘‹ å¤§å®¶å¥½! æˆ‘æ˜¯å…¨å±€ Rank: {rank} (è¿›ç¨‹ PID: {pid})
ğŸ“ æ‰€åœ¨èŠ‚ç‚¹: {node_name}

ğŸ‘€ ã€æ‰€è§ã€‘ç‰©ç†è§†é‡:
   åœ¨è¿™å°æœºå™¨ä¸Šï¼Œæˆ‘ç‰©ç†ä¸Šèƒ½çœ‹åˆ° {gpus_per_node} å¼  GPUã€‚

ğŸª‘ ã€æ‰€å¾—ã€‘æŠ¢æ¤…å­ç»“æœ:
{current_binding_info}
========================================
"""

print(info)