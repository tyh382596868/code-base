# 1.å•ä»»åŠ¡ä¸å¤šä»»åŠ¡çš„åŒºåˆ«
å•èŠ‚ç‚¹å››å¡ï¼Œ--ntasks-per-nodeå¯ä»¥æ˜¯1ä¹Ÿå¯ä»¥æ˜¯4.

æ ¸å¿ƒåŒºåˆ«åœ¨äºï¼š

--ntasks-per-node=1ï¼šåªå¯åŠ¨ 1 ä¸ª è¿›ç¨‹ï¼ˆBossï¼‰ï¼Œè¿™ 1 ä¸ªè¿›ç¨‹ç‹¬å ç®¡ç† 4 å¼ å¡ã€‚

--ntasks-per-node=4ï¼šåŒæ—¶å¯åŠ¨ 4 ä¸ª è¿›ç¨‹ï¼ˆTeamï¼‰ï¼Œé€šå¸¸é…åˆä»£ç é€»è¾‘ï¼Œè®©æ¯ä¸ªè¿›ç¨‹å„ç®¡ 1 å¼ å¡ï¼ˆå¹¶è¡Œè®­ç»ƒï¼‰ã€‚

### 1.1 å•èŠ‚ç‚¹å¤šå¡å•ä»»åŠ¡
```bash
srun -p evla2_t --gres=gpu:4 --ntasks-per-node=1 python check_slurm.py
```

**output**
```
(base) [tangyuhang@HOST-10-140-60-209 Slurm]$ srun -p evla2_t --gres=gpu:4 --ntasks-per-node=1 python check_slurm.py 
srun: job 5775208 queued and waiting for resources
srun: job 5775208 has been allocated resources
srun: Job 5775208 scheduled successfully!
Current QUOTA_TYPE is [reserved], which means the job has occupied quota in RESERVED_TOTAL under your partition.
Current PHX_PRIORITY is normal

ğŸ‘‹ å¤§å®¶å¥½! æˆ‘æ˜¯ä»»åŠ¡ ID: 0 (æ€»å…± 1 ä¸ª)ã€‚æˆ‘çš„ç³»ç»Ÿ PID æ˜¯: 118339
```

ç°è±¡ï¼šè™½ç„¶ä½ æœ‰äº† 4 å¼ å¡ï¼Œä½†å±å¹•ä¸Šåªæ‰“å°äº† 1 è¡Œ è¯ã€‚ å«ä¹‰ï¼šSlurm åªå¯åŠ¨äº†ä¸€ä¸ª Python è§£é‡Šå™¨ã€‚è¿™ä¸ª Python å¯ä»¥åœ¨å†…éƒ¨é€šè¿‡ cuda:0, cuda:1, cuda:2, cuda:3 è°ƒç”¨æ‰€æœ‰ 4 å¼ å¡ã€‚ åœºæ™¯ï¼šJupyter Lab ã€å•æœºå¤šå¡è®­ç»ƒï¼ˆDataParallelï¼‰ã€æˆ–è€…åªæ˜¯æƒ³è·‘ä¸ªç®€å•è„šæœ¬ã€‚

### 1.2å•èŠ‚ç‚¹å¤šå¡å¤šä»»åŠ¡
```bash
srun -p evla2_t --gres=gpu:4 --ntasks-per-node=4 python
```

```
(base) [tangyuhang@HOST-10-140-60-209 Slurm]$ srun -p evla2_t --gres=gpu:4 --ntasks-per-node=4 python check_slurm.py 
srun: job 5775209 queued and waiting for resources
srun: job 5775209 has been allocated resources
srun: Job 5775209 scheduled successfully!
Current QUOTA_TYPE is [reserved], which means the job has occupied quota in RESERVED_TOTAL under your partition.
Current PHX_PRIORITY is normal

ğŸ‘‹ å¤§å®¶å¥½! æˆ‘æ˜¯ä»»åŠ¡ ID: 1 (æ€»å…± 4 ä¸ª)ã€‚æˆ‘çš„ç³»ç»Ÿ PID æ˜¯: 118535
ğŸ‘‹ å¤§å®¶å¥½! æˆ‘æ˜¯ä»»åŠ¡ ID: 2 (æ€»å…± 4 ä¸ª)ã€‚æˆ‘çš„ç³»ç»Ÿ PID æ˜¯: 118536
ğŸ‘‹ å¤§å®¶å¥½! æˆ‘æ˜¯ä»»åŠ¡ ID: 3 (æ€»å…± 4 ä¸ª)ã€‚æˆ‘çš„ç³»ç»Ÿ PID æ˜¯: 118537
ğŸ‘‹ å¤§å®¶å¥½! æˆ‘æ˜¯ä»»åŠ¡ ID: 0 (æ€»å…± 4 ä¸ª)ã€‚æˆ‘çš„ç³»ç»Ÿ PID æ˜¯: 118534
```

ç°è±¡ï¼šå±å¹•ä¸Šæ‰“å°äº† 4 è¡Œ è¯ï¼Œè€Œä¸” PID éƒ½ä¸ä¸€æ ·ã€‚ å«ä¹‰ï¼šSlurm å®é™…ä¸Šå¸®ä½ æ‰§è¡Œäº† 4 æ¬¡ python check_slurm.pyã€‚ åœºæ™¯ï¼šåˆ†å¸ƒå¼è®­ç»ƒ (DDP)ã€‚é€šå¸¸ä»£ç é‡Œä¼šå†™ torch.cuda.set_device(int(os.environ["SLURM_PROCID"]))ï¼Œè®© ID ä¸º 0 çš„è¿›ç¨‹ç”¨ç¬¬ 0 å¼ å¡ï¼ŒID ä¸º 1 çš„è¿›ç¨‹ç”¨ç¬¬ 1 å¼ å¡ï¼Œä»¥æ­¤ç±»æ¨ã€‚


```bash
srun -p evla2_t --gres=gpu:4 --ntasks-per-node=8 python
```

```
(base) [tangyuhang@HOST-10-140-60-209 Slurm]$ srun -p evla2_t --gres=gpu:4 --ntasks-per-node=8 python check_slurm.py 
srun: job 5775212 queued and waiting for resources
srun: job 5775212 has been allocated resources
srun: Job 5775212 scheduled successfully!
Current QUOTA_TYPE is [reserved], which means the job has occupied quota in RESERVED_TOTAL under your partition.
Current PHX_PRIORITY is normal

ğŸ‘‹ å¤§å®¶å¥½! æˆ‘æ˜¯ä»»åŠ¡ ID: 1 (æ€»å…± 8 ä¸ª)ã€‚æˆ‘çš„ç³»ç»Ÿ PID æ˜¯: 118658
ğŸ‘‹ å¤§å®¶å¥½! æˆ‘æ˜¯ä»»åŠ¡ ID: 0 (æ€»å…± 8 ä¸ª)ã€‚æˆ‘çš„ç³»ç»Ÿ PID æ˜¯: 118657
ğŸ‘‹ å¤§å®¶å¥½! æˆ‘æ˜¯ä»»åŠ¡ ID: 2 (æ€»å…± 8 ä¸ª)ã€‚æˆ‘çš„ç³»ç»Ÿ PID æ˜¯: 118659
ğŸ‘‹ å¤§å®¶å¥½! æˆ‘æ˜¯ä»»åŠ¡ ID: 3 (æ€»å…± 8 ä¸ª)ã€‚æˆ‘çš„ç³»ç»Ÿ PID æ˜¯: 118660
ğŸ‘‹ å¤§å®¶å¥½! æˆ‘æ˜¯ä»»åŠ¡ ID: 6 (æ€»å…± 8 ä¸ª)ã€‚æˆ‘çš„ç³»ç»Ÿ PID æ˜¯: 118663
ğŸ‘‹ å¤§å®¶å¥½! æˆ‘æ˜¯ä»»åŠ¡ ID: 4 (æ€»å…± 8 ä¸ª)ã€‚æˆ‘çš„ç³»ç»Ÿ PID æ˜¯: 118661
ğŸ‘‹ å¤§å®¶å¥½! æˆ‘æ˜¯ä»»åŠ¡ ID: 7 (æ€»å…± 8 ä¸ª)ã€‚æˆ‘çš„ç³»ç»Ÿ PID æ˜¯: 118664
ğŸ‘‹ å¤§å®¶å¥½! æˆ‘æ˜¯ä»»åŠ¡ ID: 5 (æ€»å…± 8 ä¸ª)ã€‚æˆ‘çš„ç³»ç»Ÿ PID æ˜¯: 118662
```

### 1.3 å¤šèŠ‚ç‚¹å¤šå¡å¤šä»»åŠ¡
```bash
srun -p evla2_t --gres=gpu:4 --nodes=2 --ntasks-per-node=2 python check_slurm.py
```

```
(base) [tangyuhang@HOST-10-140-60-209 Slurm]$ srun -p evla2_t --gres=gpu:4 --nodes=2 --ntasks-per-node=2 python check_slurm.py 
srun: job 5775235 queued and waiting for resources
srun: job 5775235 has been allocated resources
srun: Job 5775235 scheduled successfully!
Current QUOTA_TYPE is [reserved], which means the job has occupied quota in RESERVED_TOTAL under your partition.
Current PHX_PRIORITY is normal

ğŸ‘‹ å¤§å®¶å¥½! æˆ‘æ˜¯ä»»åŠ¡ ID: 2 (æ€»å…± 4 ä¸ª)ã€‚æˆ‘çš„ç³»ç»Ÿ PID æ˜¯: 121368
ğŸ‘‹ å¤§å®¶å¥½! æˆ‘æ˜¯ä»»åŠ¡ ID: 3 (æ€»å…± 4 ä¸ª)ã€‚æˆ‘çš„ç³»ç»Ÿ PID æ˜¯: 121369
ğŸ‘‹ å¤§å®¶å¥½! æˆ‘æ˜¯ä»»åŠ¡ ID: 1 (æ€»å…± 4 ä¸ª)ã€‚æˆ‘çš„ç³»ç»Ÿ PID æ˜¯: 108969
ğŸ‘‹ å¤§å®¶å¥½! æˆ‘æ˜¯ä»»åŠ¡ ID: 0 (æ€»å…± 4 ä¸ª)ã€‚æˆ‘çš„ç³»ç»Ÿ PID æ˜¯: 108968
```

```bash
srun -p evla2_t --gres=gpu:4 --nodes=2 --ntasks-per-node=8 python check_slurm.py
```

```
(base) [tangyuhang@HOST-10-140-60-209 Slurm]$ srun -p evla2_t --gres=gpu:4 --nodes=2 --ntasks-per-node=8 python check_slurm.py 
srun: job 5775241 queued and waiting for resources
srun: job 5775241 has been allocated resources
srun: Job 5775241 scheduled successfully!
Current QUOTA_TYPE is [reserved], which means the job has occupied quota in RESERVED_TOTAL under your partition.
Current PHX_PRIORITY is normal

ğŸ‘‹ å¤§å®¶å¥½! æˆ‘æ˜¯ä»»åŠ¡ ID: 11 (æ€»å…± 16 ä¸ª)ã€‚æˆ‘çš„ç³»ç»Ÿ PID æ˜¯: 121758
ğŸ‘‹ å¤§å®¶å¥½! æˆ‘æ˜¯ä»»åŠ¡ ID: 12 (æ€»å…± 16 ä¸ª)ã€‚æˆ‘çš„ç³»ç»Ÿ PID æ˜¯: 121759
ğŸ‘‹ å¤§å®¶å¥½! æˆ‘æ˜¯ä»»åŠ¡ ID: 8 (æ€»å…± 16 ä¸ª)ã€‚æˆ‘çš„ç³»ç»Ÿ PID æ˜¯: 121755
ğŸ‘‹ å¤§å®¶å¥½! æˆ‘æ˜¯ä»»åŠ¡ ID: 10 (æ€»å…± 16 ä¸ª)ã€‚æˆ‘çš„ç³»ç»Ÿ PID æ˜¯: 121757
ğŸ‘‹ å¤§å®¶å¥½! æˆ‘æ˜¯ä»»åŠ¡ ID: 15 (æ€»å…± 16 ä¸ª)ã€‚æˆ‘çš„ç³»ç»Ÿ PID æ˜¯: 121762
ğŸ‘‹ å¤§å®¶å¥½! æˆ‘æ˜¯ä»»åŠ¡ ID: 14 (æ€»å…± 16 ä¸ª)ã€‚æˆ‘çš„ç³»ç»Ÿ PID æ˜¯: 121761
ğŸ‘‹ å¤§å®¶å¥½! æˆ‘æ˜¯ä»»åŠ¡ ID: 9 (æ€»å…± 16 ä¸ª)ã€‚æˆ‘çš„ç³»ç»Ÿ PID æ˜¯: 121756
ğŸ‘‹ å¤§å®¶å¥½! æˆ‘æ˜¯ä»»åŠ¡ ID: 13 (æ€»å…± 16 ä¸ª)ã€‚æˆ‘çš„ç³»ç»Ÿ PID æ˜¯: 121760
ğŸ‘‹ å¤§å®¶å¥½! æˆ‘æ˜¯ä»»åŠ¡ ID: 5 (æ€»å…± 16 ä¸ª)ã€‚æˆ‘çš„ç³»ç»Ÿ PID æ˜¯: 109205
ğŸ‘‹ å¤§å®¶å¥½! æˆ‘æ˜¯ä»»åŠ¡ ID: 4 (æ€»å…± 16 ä¸ª)ã€‚æˆ‘çš„ç³»ç»Ÿ PID æ˜¯: 109195
ğŸ‘‹ å¤§å®¶å¥½! æˆ‘æ˜¯ä»»åŠ¡ ID: 6 (æ€»å…± 16 ä¸ª)ã€‚æˆ‘çš„ç³»ç»Ÿ PID æ˜¯: 109206
ğŸ‘‹ å¤§å®¶å¥½! æˆ‘æ˜¯ä»»åŠ¡ ID: 7 (æ€»å…± 16 ä¸ª)ã€‚æˆ‘çš„ç³»ç»Ÿ PID æ˜¯: 109207
ğŸ‘‹ å¤§å®¶å¥½! æˆ‘æ˜¯ä»»åŠ¡ ID: 0 (æ€»å…± 16 ä¸ª)ã€‚æˆ‘çš„ç³»ç»Ÿ PID æ˜¯: 109191
ğŸ‘‹ å¤§å®¶å¥½! æˆ‘æ˜¯ä»»åŠ¡ ID: 1 (æ€»å…± 16 ä¸ª)ã€‚æˆ‘çš„ç³»ç»Ÿ PID æ˜¯: 109192
ğŸ‘‹ å¤§å®¶å¥½! æˆ‘æ˜¯ä»»åŠ¡ ID: 3 (æ€»å…± 16 ä¸ª)ã€‚æˆ‘çš„ç³»ç»Ÿ PID æ˜¯: 109194
ğŸ‘‹ å¤§å®¶å¥½! æˆ‘æ˜¯ä»»åŠ¡ ID: 2 (æ€»å…± 16 ä¸ª)ã€‚æˆ‘çš„ç³»ç»Ÿ PID æ˜¯: 109193
```

# 2. ä»»åŠ¡æ•°é‡ä¸GPUæ•°é‡çš„çš„è”ç³»

æ¯ä¸ªèŠ‚ç‚¹ä¸Šçš„ä»»åŠ¡æ•°é‡ä¸GPUæ•°é‡ä¸ä¸€è‡´ä¸ä¼šæŠ¥é”™ï¼Œä½†æ˜¯æœ€å¥½ä¿æŒä¸€è‡´ï¼Œä¸ºæ¯ä¸ªä»»åŠ¡åˆ†é…ä¸€ä¸ªGPUã€‚
æ¯ä¸ªèŠ‚ç‚¹çš„ä»»åŠ¡æ•°é‡ä¸æ¯ä¸ªèŠ‚ç‚¹çš„GPUæ•°é‡ä¸ä¸€è‡´æ—¶ï¼š
    "--ntasks-per-node " > "--gres=gpu: " æ—¶ï¼šå¤šä¸ªä»»åŠ¡å…±ç”¨ä¸€ä¸ªGPUï¼ŒGPUæ˜¾å­˜å¯èƒ½ä¼šçˆ†ã€‚
    "--ntasks-per-node " < "--gres=gpu: " æ—¶ï¼šæœ‰GPUä¼šç©ºé—²æ²¡è¢«åˆ©ç”¨ã€‚

1. Slurm åˆ°åº•å¯åŠ¨äº†å‡ ä¸ª Python è¿›ç¨‹ï¼Ÿ (çœ‹æ‰“å°äº†å‡ æ¬¡)

2. æ¯ä¸ªè¿›ç¨‹åˆ°åº•èƒ½ç”¨å‡ å¼ å¡ï¼Ÿ (çœ‹ "GPU æ•°é‡" æ˜¯ 1 è¿˜æ˜¯ 4)

## 2.1 å®éªŒ Aï¼šå•ä»»åŠ¡ï¼Œå››å¼ å¡
```bash
srun -p evla2_t --gres=gpu:4 --ntasks-per-node=1 python check_slurm_gpu.py
```

```
(openvla-oft) [tangyuhang@HOST-10-140-60-209 Slurm]$ srun -p evla2_t --gres=gpu:4 --ntasks-per-node=1 python check_slurm_gpu.py 
srun: job 5775278 queued and waiting for resources
srun: job 5775278 has been allocated resources
srun: Job 5775278 scheduled successfully!
Current QUOTA_TYPE is [reserved], which means the job has occupied quota in RESERVED_TOTAL under your partition.
Current PHX_PRIORITY is normal


========================================
ğŸ‘‹ æˆ‘æ˜¯ä»»åŠ¡ ID: 0 (å…± 1 ä¸ª)
ğŸ“ æ‰€åœ¨èŠ‚ç‚¹: HOST-10-140-66-41 | è¿›ç¨‹ PID: 60599
ğŸ‘€ èƒ½çœ‹åˆ°çš„ GPU æ•°é‡: 4 ä¸ª
   -> GPU 0: NVIDIA A800-SXM4-80GB (æ˜¾å­˜: 79.3 GB)
   -> GPU 1: NVIDIA A800-SXM4-80GB (æ˜¾å­˜: 79.3 GB)
   -> GPU 2: NVIDIA A800-SXM4-80GB (æ˜¾å­˜: 79.3 GB)
   -> GPU 3: NVIDIA A800-SXM4-80GB (æ˜¾å­˜: 79.3 GB)
========================================
```
æ‰“å°æ¬¡æ•°ï¼š1 æ¬¡ã€‚

å†…å®¹ï¼šè¿™ 1 ä¸ªè¿›ç¨‹è¯´â€œæˆ‘çœ‹åˆ°äº† 4 ä¸ª GPUâ€ã€‚

æ„ä¹‰ï¼šè¿™å°±æ˜¯æ‚¨è·‘ Jupyter æ—¶çš„çŠ¶æ€ã€‚æ‚¨çš„ä»£ç æ‹¥æœ‰å…¨éƒ¨ 4 å¼ å¡çš„æ§åˆ¶æƒã€‚

## 2.2 å®éªŒ Bï¼šå››ä»»åŠ¡ï¼Œå››å¼ å¡

éš”ç¦»æ¨¡å¼ï¼ˆå¸¸è§ï¼‰ï¼šä»»åŠ¡ 0 è¯´â€œæˆ‘åªçœ‹åˆ° 1 ä¸ª GPUï¼ˆå…¶å®æ˜¯ç‰©ç†å¡0ï¼‰â€ï¼Œä»»åŠ¡ 1 è¯´â€œæˆ‘ä¹Ÿåªçœ‹åˆ° 1 ä¸ª GPUï¼ˆå…¶å®æ˜¯ç‰©ç†å¡1ï¼‰â€ã€‚è¿™æ˜¯æœ€å®Œç¾çš„ DDP çŠ¶æ€ï¼Œäº’ä¸å¹²æ‰°ã€‚

ééš”ç¦»æ¨¡å¼ï¼šä»»åŠ¡ 0 è¯´â€œæˆ‘çœ‹åˆ°äº† 4 ä¸ª GPUâ€ï¼Œä»»åŠ¡ 1 ä¹Ÿè¯´â€œæˆ‘çœ‹åˆ°äº† 4 ä¸ª GPUâ€ã€‚è¿™ç§æƒ…å†µä¸‹ï¼Œæ‚¨éœ€è¦åœ¨ä»£ç é‡Œæ‰‹åŠ¨æŒ‡å®š torch.cuda.set_device(local_rank) ä¹Ÿå°±æ˜¯â€œè™½ç„¶æˆ‘çœ‹åˆ°äº† 4 ä¸ªï¼Œä½†æˆ‘åªç”¨ç¬¬ X ä¸ªâ€ã€‚

### 2.2.1 ééš”ç¦»æ¨¡å¼

```bash
srun -p evla2_t --gres=gpu:4 --ntasks-per-node=2 python check_slurm_gpu.py
```

```
(openvla-oft) [tangyuhang@HOST-10-140-60-209 Slurm]$ srun -p evla2_t --gres=gpu:4 --ntasks-per-node=2 python check_slurm_gpu.py 
srun: job 5775287 queued and waiting for resources
srun: job 5775287 has been allocated resources
srun: Job 5775287 scheduled successfully!
Current QUOTA_TYPE is [reserved], which means the job has occupied quota in RESERVED_TOTAL under your partition.
Current PHX_PRIORITY is normal


========================================
ğŸ‘‹ æˆ‘æ˜¯ä»»åŠ¡ ID: 1 (å…± 2 ä¸ª)
ğŸ“ æ‰€åœ¨èŠ‚ç‚¹: HOST-10-140-60-136 | è¿›ç¨‹ PID: 25386
ğŸ‘€ èƒ½çœ‹åˆ°çš„ GPU æ•°é‡: 4 ä¸ª
   -> GPU 0: NVIDIA A800-SXM4-80GB (æ˜¾å­˜: 79.3 GB)
   -> GPU 1: NVIDIA A800-SXM4-80GB (æ˜¾å­˜: 79.3 GB)
   -> GPU 2: NVIDIA A800-SXM4-80GB (æ˜¾å­˜: 79.3 GB)
   -> GPU 3: NVIDIA A800-SXM4-80GB (æ˜¾å­˜: 79.3 GB)
========================================

========================================
ğŸ‘‹ æˆ‘æ˜¯ä»»åŠ¡ ID: 0 (å…± 2 ä¸ª)
ğŸ“ æ‰€åœ¨èŠ‚ç‚¹: HOST-10-140-60-136 | è¿›ç¨‹ PID: 25385
ğŸ‘€ èƒ½çœ‹åˆ°çš„ GPU æ•°é‡: 4 ä¸ª
   -> GPU 0: NVIDIA A800-SXM4-80GB (æ˜¾å­˜: 79.3 GB)
   -> GPU 1: NVIDIA A800-SXM4-80GB (æ˜¾å­˜: 79.3 GB)
   -> GPU 2: NVIDIA A800-SXM4-80GB (æ˜¾å­˜: 79.3 GB)
   -> GPU 3: NVIDIA A800-SXM4-80GB (æ˜¾å­˜: 79.3 GB)
========================================

```

```bash
srun -p evla2_t --gres=gpu:4 --ntasks-per-node=4 python check_slurm_gpu.py
```

```
(openvla-oft) [tangyuhang@HOST-10-140-60-209 Slurm]$ srun -p evla2_t --gres=gpu:4 --ntasks-per-node=4 python check_slurm_gpu.py 
srun: job 5775297 queued and waiting for resources
srun: job 5775297 has been allocated resources
srun: Job 5775297 scheduled successfully!
Current QUOTA_TYPE is [reserved], which means the job has occupied quota in RESERVED_TOTAL under your partition.
Current PHX_PRIORITY is normal


========================================
ğŸ‘‹ æˆ‘æ˜¯ä»»åŠ¡ ID: 1 (å…± 4 ä¸ª)
ğŸ“ æ‰€åœ¨èŠ‚ç‚¹: HOST-10-140-60-136 | è¿›ç¨‹ PID: 26390
ğŸ‘€ èƒ½çœ‹åˆ°çš„ GPU æ•°é‡: 4 ä¸ª
   -> GPU 0: NVIDIA A800-SXM4-80GB (æ˜¾å­˜: 79.3 GB)
   -> GPU 1: NVIDIA A800-SXM4-80GB (æ˜¾å­˜: 79.3 GB)
   -> GPU 2: NVIDIA A800-SXM4-80GB (æ˜¾å­˜: 79.3 GB)
   -> GPU 3: NVIDIA A800-SXM4-80GB (æ˜¾å­˜: 79.3 GB)
========================================

========================================
ğŸ‘‹ æˆ‘æ˜¯ä»»åŠ¡ ID: 3 (å…± 4 ä¸ª)
ğŸ“ æ‰€åœ¨èŠ‚ç‚¹: HOST-10-140-60-136 | è¿›ç¨‹ PID: 26392
ğŸ‘€ èƒ½çœ‹åˆ°çš„ GPU æ•°é‡: 4 ä¸ª
   -> GPU 0: NVIDIA A800-SXM4-80GB (æ˜¾å­˜: 79.3 GB)
   -> GPU 1: NVIDIA A800-SXM4-80GB (æ˜¾å­˜: 79.3 GB)
   -> GPU 2: NVIDIA A800-SXM4-80GB (æ˜¾å­˜: 79.3 GB)
   -> GPU 3: NVIDIA A800-SXM4-80GB (æ˜¾å­˜: 79.3 GB)
========================================

========================================
ğŸ‘‹ æˆ‘æ˜¯ä»»åŠ¡ ID: 2 (å…± 4 ä¸ª)
ğŸ“ æ‰€åœ¨èŠ‚ç‚¹: HOST-10-140-60-136 | è¿›ç¨‹ PID: 26391
ğŸ‘€ èƒ½çœ‹åˆ°çš„ GPU æ•°é‡: 4 ä¸ª
   -> GPU 0: NVIDIA A800-SXM4-80GB (æ˜¾å­˜: 79.3 GB)
   -> GPU 1: NVIDIA A800-SXM4-80GB (æ˜¾å­˜: 79.3 GB)
   -> GPU 2: NVIDIA A800-SXM4-80GB (æ˜¾å­˜: 79.3 GB)
   -> GPU 3: NVIDIA A800-SXM4-80GB (æ˜¾å­˜: 79.3 GB)
========================================

========================================
ğŸ‘‹ æˆ‘æ˜¯ä»»åŠ¡ ID: 0 (å…± 4 ä¸ª)
ğŸ“ æ‰€åœ¨èŠ‚ç‚¹: HOST-10-140-60-136 | è¿›ç¨‹ PID: 26389
ğŸ‘€ èƒ½çœ‹åˆ°çš„ GPU æ•°é‡: 4 ä¸ª
   -> GPU 0: NVIDIA A800-SXM4-80GB (æ˜¾å­˜: 79.3 GB)
   -> GPU 1: NVIDIA A800-SXM4-80GB (æ˜¾å­˜: 79.3 GB)
   -> GPU 2: NVIDIA A800-SXM4-80GB (æ˜¾å­˜: 79.3 GB)
   -> GPU 3: NVIDIA A800-SXM4-80GB (æ˜¾å­˜: 79.3 GB)
========================================
```

æ¯ä¸ªä»»åŠ¡ï¼ˆTask/Processï¼‰éƒ½èƒ½â€œçœ‹è§â€å¹¶ä¸”ç†è®ºä¸Šèƒ½â€œè®¿é—®â€æ‰€æœ‰çš„ GPUã€‚

è¿™å°±å¸¦æ¥äº†ä¸€ä¸ªæå¤§çš„é£é™©ï¼Œå«åš**â€œæŠ¢æ¤…å­â€é—®é¢˜**ã€‚å¦‚æœä¸åŠ æ§åˆ¶ï¼Œæ‰€æœ‰ 4 ä¸ªä»»åŠ¡éƒ½ä¼šé»˜è®¤å»æŠ¢ç¬¬ 0 å· GPUï¼Œå¯¼è‡´ç¬¬ 0 å·æ˜¾å­˜çˆ†ç‚¸ï¼Œè€Œå‰©ä¸‹ 3 å¼ å¡åœ¨æ—è¾¹å›´è§‚ã€‚

1. ä¸ºä»€ä¹ˆä¼šè¿™æ ·ï¼Ÿï¼ˆç°çŠ¶åˆ†æï¼‰
å¯è§æ€§ï¼šæ‚¨çš„æ—¥å¿—æ˜¾ç¤º GPU 0 åˆ° GPU 3 å‡ºç°åœ¨äº†æ‰€æœ‰ 4 ä¸ªä»»åŠ¡çš„è§†é‡é‡Œã€‚è¯´æ˜ Slurm å¹¶æ²¡æœ‰åœ¨ç³»ç»Ÿå±‚é¢åšâ€œæ˜¾å¡éš”ç¦»â€ï¼ˆCgroup Isolationï¼‰ã€‚

é»˜è®¤è¡Œä¸ºï¼šåœ¨ PyTorch ä¸­ï¼Œå¦‚æœæ‚¨ä¸ç‰¹æ„æŒ‡å®šï¼Œä»£ç é»˜è®¤éƒ½ä¼šä½¿ç”¨ cuda:0ï¼ˆä¹Ÿå°±æ˜¯åˆ—è¡¨é‡Œçš„ç¬¬ä¸€ä¸ªè®¾å¤‡ï¼‰ã€‚

åæœï¼š å¦‚æœä¸å†™ä»£ç è¿›è¡Œåˆ†é…ï¼ŒTask 0, 1, 2, 3 å…¨éƒ½ä¼šæŒ¤åˆ°ç‰©ç†ä¸Šçš„ GPU 0 ä¸Šå»è·‘ï¼Œç¬é—´ OOMï¼ˆæ˜¾å­˜æº¢å‡ºï¼‰ã€‚

### 2.2.2 éš”ç¦»æ¨¡å¼
pytorch è®­ç»ƒä»£ç é‡ŒåŠ ä¸Šä»¥ä¸‹

```python
# 1. è®¡ç®—å½“å‰è¿›ç¨‹åº”è¯¥åå“ªå¼ æ¤…å­ï¼ˆè®¡ç®— local_rankï¼‰
local_rank = rank - gpus_per_node * (rank // gpus_per_node)

# 2. ã€æ ¸å¿ƒçš„ä¸€è¡Œã€‘å¼ºåˆ¶è®¾å®šå½“å‰è¿›ç¨‹çš„é»˜è®¤è®¾å¤‡
torch.cuda.set_device(local_rank)

# 3. å°†æ¨¡å‹æ¬åˆ°æŒ‡å®šçš„æ¤…å­ä¸Š
model = Net().to(local_rank)
```
```bash
srun -p evla2_t --gres=gpu:4 --ntasks-per-node=4 python check_slurm_gpu_fix.py 
```

```
(openvla-oft) [tangyuhang@HOST-10-140-60-209 Slurm]$ srun -p evla2_t --gres=gpu:4 --ntasks-per-node=4 python check_slurm_gpu_fix.py 
srun: job 5775958 queued and waiting for resources
srun: job 5775958 has been allocated resources
srun: Job 5775958 scheduled successfully!
Current QUOTA_TYPE is [reserved], which means the job has occupied quota in RESERVED_TOTAL under your partition.
Current PHX_PRIORITY is normal


========================================
ğŸ‘‹ å¤§å®¶å¥½! æˆ‘æ˜¯å…¨å±€ Rank: 2 (è¿›ç¨‹ PID: 46285)
ğŸ“ æ‰€åœ¨èŠ‚ç‚¹: HOST-10-140-60-136

ğŸ‘€ ã€æ‰€è§ã€‘ç‰©ç†è§†é‡:
   åœ¨è¿™å°æœºå™¨ä¸Šï¼Œæˆ‘ç‰©ç†ä¸Šèƒ½çœ‹åˆ° 4 å¼  GPUã€‚

ğŸª‘ ã€æ‰€å¾—ã€‘æŠ¢æ¤…å­ç»“æœ:
âœ… å·²ç»‘å®šé€»è¾‘åº§ä½: 2
   -> éªŒè¯ current_device(): 2
   -> éªŒè¯ Tensor ä½ç½®: cuda:2
   -> ç¡¬ä»¶å‹å·: NVIDIA A800-SXM4-80GB
========================================


========================================
ğŸ‘‹ å¤§å®¶å¥½! æˆ‘æ˜¯å…¨å±€ Rank: 3 (è¿›ç¨‹ PID: 46286)
ğŸ“ æ‰€åœ¨èŠ‚ç‚¹: HOST-10-140-60-136

ğŸ‘€ ã€æ‰€è§ã€‘ç‰©ç†è§†é‡:
   åœ¨è¿™å°æœºå™¨ä¸Šï¼Œæˆ‘ç‰©ç†ä¸Šèƒ½çœ‹åˆ° 4 å¼  GPUã€‚

ğŸª‘ ã€æ‰€å¾—ã€‘æŠ¢æ¤…å­ç»“æœ:
âœ… å·²ç»‘å®šé€»è¾‘åº§ä½: 3
   -> éªŒè¯ current_device(): 3
   -> éªŒè¯ Tensor ä½ç½®: cuda:3
   -> ç¡¬ä»¶å‹å·: NVIDIA A800-SXM4-80GB
========================================


========================================
ğŸ‘‹ å¤§å®¶å¥½! æˆ‘æ˜¯å…¨å±€ Rank: 0 (è¿›ç¨‹ PID: 46283)
ğŸ“ æ‰€åœ¨èŠ‚ç‚¹: HOST-10-140-60-136

ğŸ‘€ ã€æ‰€è§ã€‘ç‰©ç†è§†é‡:
   åœ¨è¿™å°æœºå™¨ä¸Šï¼Œæˆ‘ç‰©ç†ä¸Šèƒ½çœ‹åˆ° 4 å¼  GPUã€‚

ğŸª‘ ã€æ‰€å¾—ã€‘æŠ¢æ¤…å­ç»“æœ:
âœ… å·²ç»‘å®šé€»è¾‘åº§ä½: 0
   -> éªŒè¯ current_device(): 0
   -> éªŒè¯ Tensor ä½ç½®: cuda:0
   -> ç¡¬ä»¶å‹å·: NVIDIA A800-SXM4-80GB
========================================


========================================
ğŸ‘‹ å¤§å®¶å¥½! æˆ‘æ˜¯å…¨å±€ Rank: 1 (è¿›ç¨‹ PID: 46284)
ğŸ“ æ‰€åœ¨èŠ‚ç‚¹: HOST-10-140-60-136

ğŸ‘€ ã€æ‰€è§ã€‘ç‰©ç†è§†é‡:
   åœ¨è¿™å°æœºå™¨ä¸Šï¼Œæˆ‘ç‰©ç†ä¸Šèƒ½çœ‹åˆ° 4 å¼  GPUã€‚

ğŸª‘ ã€æ‰€å¾—ã€‘æŠ¢æ¤…å­ç»“æœ:
âœ… å·²ç»‘å®šé€»è¾‘åº§ä½: 1
   -> éªŒè¯ current_device(): 1
   -> éªŒè¯ Tensor ä½ç½®: cuda:1
   -> ç¡¬ä»¶å‹å·: NVIDIA A800-SXM4-80GB
========================================
```
```bash
srun -p evla2_t --nodes=2 --gres=gpu:4 --ntasks-per-node=4 python check_slurm_gpu_fix.py
```

```
(openvla-oft) [tangyuhang@HOST-10-140-60-209 Slurm]$ srun -p evla2_t --nodes=2 --gres=gpu:4 --ntasks-per-node=4 python check_slurm_gpu_fix.py 
srun: job 5775967 queued and waiting for resources
srun: job 5775967 has been allocated resources
srun: Job 5775967 scheduled successfully!
Current QUOTA_TYPE is [reserved], which means the job has occupied quota in RESERVED_TOTAL under your partition.
Current PHX_PRIORITY is normal


========================================
ğŸ‘‹ å¤§å®¶å¥½! æˆ‘æ˜¯å…¨å±€ Rank: 4 (è¿›ç¨‹ PID: 125481)
ğŸ“ æ‰€åœ¨èŠ‚ç‚¹: HOST-10-140-60-146

ğŸ‘€ ã€æ‰€è§ã€‘ç‰©ç†è§†é‡:
   åœ¨è¿™å°æœºå™¨ä¸Šï¼Œæˆ‘ç‰©ç†ä¸Šèƒ½çœ‹åˆ° 4 å¼  GPUã€‚

ğŸª‘ ã€æ‰€å¾—ã€‘æŠ¢æ¤…å­ç»“æœ:
âœ… å·²ç»‘å®šé€»è¾‘åº§ä½: 0
   -> éªŒè¯ current_device(): 0
   -> éªŒè¯ Tensor ä½ç½®: cuda:0
   -> ç¡¬ä»¶å‹å·: NVIDIA A800-SXM4-80GB
========================================


========================================
ğŸ‘‹ å¤§å®¶å¥½! æˆ‘æ˜¯å…¨å±€ Rank: 5 (è¿›ç¨‹ PID: 125482)
ğŸ“ æ‰€åœ¨èŠ‚ç‚¹: HOST-10-140-60-146

ğŸ‘€ ã€æ‰€è§ã€‘ç‰©ç†è§†é‡:
   åœ¨è¿™å°æœºå™¨ä¸Šï¼Œæˆ‘ç‰©ç†ä¸Šèƒ½çœ‹åˆ° 4 å¼  GPUã€‚

ğŸª‘ ã€æ‰€å¾—ã€‘æŠ¢æ¤…å­ç»“æœ:
âœ… å·²ç»‘å®šé€»è¾‘åº§ä½: 1
   -> éªŒè¯ current_device(): 1
   -> éªŒè¯ Tensor ä½ç½®: cuda:1
   -> ç¡¬ä»¶å‹å·: NVIDIA A800-SXM4-80GB
========================================


========================================
ğŸ‘‹ å¤§å®¶å¥½! æˆ‘æ˜¯å…¨å±€ Rank: 7 (è¿›ç¨‹ PID: 125484)
ğŸ“ æ‰€åœ¨èŠ‚ç‚¹: HOST-10-140-60-146

ğŸ‘€ ã€æ‰€è§ã€‘ç‰©ç†è§†é‡:
   åœ¨è¿™å°æœºå™¨ä¸Šï¼Œæˆ‘ç‰©ç†ä¸Šèƒ½çœ‹åˆ° 4 å¼  GPUã€‚

ğŸª‘ ã€æ‰€å¾—ã€‘æŠ¢æ¤…å­ç»“æœ:
âœ… å·²ç»‘å®šé€»è¾‘åº§ä½: 3
   -> éªŒè¯ current_device(): 3
   -> éªŒè¯ Tensor ä½ç½®: cuda:3
   -> ç¡¬ä»¶å‹å·: NVIDIA A800-SXM4-80GB
========================================


========================================
ğŸ‘‹ å¤§å®¶å¥½! æˆ‘æ˜¯å…¨å±€ Rank: 6 (è¿›ç¨‹ PID: 125483)
ğŸ“ æ‰€åœ¨èŠ‚ç‚¹: HOST-10-140-60-146

ğŸ‘€ ã€æ‰€è§ã€‘ç‰©ç†è§†é‡:
   åœ¨è¿™å°æœºå™¨ä¸Šï¼Œæˆ‘ç‰©ç†ä¸Šèƒ½çœ‹åˆ° 4 å¼  GPUã€‚

ğŸª‘ ã€æ‰€å¾—ã€‘æŠ¢æ¤…å­ç»“æœ:
âœ… å·²ç»‘å®šé€»è¾‘åº§ä½: 2
   -> éªŒè¯ current_device(): 2
   -> éªŒè¯ Tensor ä½ç½®: cuda:2
   -> ç¡¬ä»¶å‹å·: NVIDIA A800-SXM4-80GB
========================================


========================================
ğŸ‘‹ å¤§å®¶å¥½! æˆ‘æ˜¯å…¨å±€ Rank: 3 (è¿›ç¨‹ PID: 47285)
ğŸ“ æ‰€åœ¨èŠ‚ç‚¹: HOST-10-140-60-136

ğŸ‘€ ã€æ‰€è§ã€‘ç‰©ç†è§†é‡:
   åœ¨è¿™å°æœºå™¨ä¸Šï¼Œæˆ‘ç‰©ç†ä¸Šèƒ½çœ‹åˆ° 4 å¼  GPUã€‚

ğŸª‘ ã€æ‰€å¾—ã€‘æŠ¢æ¤…å­ç»“æœ:
âœ… å·²ç»‘å®šé€»è¾‘åº§ä½: 3
   -> éªŒè¯ current_device(): 3
   -> éªŒè¯ Tensor ä½ç½®: cuda:3
   -> ç¡¬ä»¶å‹å·: NVIDIA A800-SXM4-80GB
========================================


========================================
ğŸ‘‹ å¤§å®¶å¥½! æˆ‘æ˜¯å…¨å±€ Rank: 2 (è¿›ç¨‹ PID: 47284)
ğŸ“ æ‰€åœ¨èŠ‚ç‚¹: HOST-10-140-60-136

ğŸ‘€ ã€æ‰€è§ã€‘ç‰©ç†è§†é‡:
   åœ¨è¿™å°æœºå™¨ä¸Šï¼Œæˆ‘ç‰©ç†ä¸Šèƒ½çœ‹åˆ° 4 å¼  GPUã€‚

ğŸª‘ ã€æ‰€å¾—ã€‘æŠ¢æ¤…å­ç»“æœ:
âœ… å·²ç»‘å®šé€»è¾‘åº§ä½: 2
   -> éªŒè¯ current_device(): 2
   -> éªŒè¯ Tensor ä½ç½®: cuda:2
   -> ç¡¬ä»¶å‹å·: NVIDIA A800-SXM4-80GB
========================================


========================================
ğŸ‘‹ å¤§å®¶å¥½! æˆ‘æ˜¯å…¨å±€ Rank: 1 (è¿›ç¨‹ PID: 47283)
ğŸ“ æ‰€åœ¨èŠ‚ç‚¹: HOST-10-140-60-136

ğŸ‘€ ã€æ‰€è§ã€‘ç‰©ç†è§†é‡:
   åœ¨è¿™å°æœºå™¨ä¸Šï¼Œæˆ‘ç‰©ç†ä¸Šèƒ½çœ‹åˆ° 4 å¼  GPUã€‚

ğŸª‘ ã€æ‰€å¾—ã€‘æŠ¢æ¤…å­ç»“æœ:
âœ… å·²ç»‘å®šé€»è¾‘åº§ä½: 1
   -> éªŒè¯ current_device(): 1
   -> éªŒè¯ Tensor ä½ç½®: cuda:1
   -> ç¡¬ä»¶å‹å·: NVIDIA A800-SXM4-80GB
========================================


========================================
ğŸ‘‹ å¤§å®¶å¥½! æˆ‘æ˜¯å…¨å±€ Rank: 0 (è¿›ç¨‹ PID: 47282)
ğŸ“ æ‰€åœ¨èŠ‚ç‚¹: HOST-10-140-60-136

ğŸ‘€ ã€æ‰€è§ã€‘ç‰©ç†è§†é‡:
   åœ¨è¿™å°æœºå™¨ä¸Šï¼Œæˆ‘ç‰©ç†ä¸Šèƒ½çœ‹åˆ° 4 å¼  GPUã€‚

ğŸª‘ ã€æ‰€å¾—ã€‘æŠ¢æ¤…å­ç»“æœ:
âœ… å·²ç»‘å®šé€»è¾‘åº§ä½: 0
   -> éªŒè¯ current_device(): 0
   -> éªŒè¯ Tensor ä½ç½®: cuda:0
   -> ç¡¬ä»¶å‹å·: NVIDIA A800-SXM4-80GB
========================================

(openvla-oft) [tangyuhang@HOST-10-140-60-209 Slurm]$ srun -p evla2_t --nodes=2 --gres=gpu:2 --ntasks-per-node=4 python check_slurm_gpu_fix.py 
srun: job 5775971 queued and waiting for resources
srun: job 5775971 has been allocated resources
srun: Job 5775971 scheduled successfully!
Current QUOTA_TYPE is [reserved], which means the job has occupied quota in RESERVED_TOTAL under your partition.
Current PHX_PRIORITY is normal


========================================
ğŸ‘‹ å¤§å®¶å¥½! æˆ‘æ˜¯å…¨å±€ Rank: 4 (è¿›ç¨‹ PID: 49317)
ğŸ“ æ‰€åœ¨èŠ‚ç‚¹: HOST-10-140-60-136

ğŸ‘€ ã€æ‰€è§ã€‘ç‰©ç†è§†é‡:
   åœ¨è¿™å°æœºå™¨ä¸Šï¼Œæˆ‘ç‰©ç†ä¸Šèƒ½çœ‹åˆ° 2 å¼  GPUã€‚

ğŸª‘ ã€æ‰€å¾—ã€‘æŠ¢æ¤…å­ç»“æœ:
âœ… å·²ç»‘å®šé€»è¾‘åº§ä½: 0
   -> éªŒè¯ current_device(): 0
   -> éªŒè¯ Tensor ä½ç½®: cuda:0
   -> ç¡¬ä»¶å‹å·: NVIDIA A800-SXM4-80GB
========================================


========================================
ğŸ‘‹ å¤§å®¶å¥½! æˆ‘æ˜¯å…¨å±€ Rank: 5 (è¿›ç¨‹ PID: 49318)
ğŸ“ æ‰€åœ¨èŠ‚ç‚¹: HOST-10-140-60-136

ğŸ‘€ ã€æ‰€è§ã€‘ç‰©ç†è§†é‡:
   åœ¨è¿™å°æœºå™¨ä¸Šï¼Œæˆ‘ç‰©ç†ä¸Šèƒ½çœ‹åˆ° 2 å¼  GPUã€‚

ğŸª‘ ã€æ‰€å¾—ã€‘æŠ¢æ¤…å­ç»“æœ:
âœ… å·²ç»‘å®šé€»è¾‘åº§ä½: 1
   -> éªŒè¯ current_device(): 1
   -> éªŒè¯ Tensor ä½ç½®: cuda:1
   -> ç¡¬ä»¶å‹å·: NVIDIA A800-SXM4-80GB
========================================


========================================
ğŸ‘‹ å¤§å®¶å¥½! æˆ‘æ˜¯å…¨å±€ Rank: 6 (è¿›ç¨‹ PID: 49319)
ğŸ“ æ‰€åœ¨èŠ‚ç‚¹: HOST-10-140-60-136

ğŸ‘€ ã€æ‰€è§ã€‘ç‰©ç†è§†é‡:
   åœ¨è¿™å°æœºå™¨ä¸Šï¼Œæˆ‘ç‰©ç†ä¸Šèƒ½çœ‹åˆ° 2 å¼  GPUã€‚

ğŸª‘ ã€æ‰€å¾—ã€‘æŠ¢æ¤…å­ç»“æœ:
âœ… å·²ç»‘å®šé€»è¾‘åº§ä½: 0
   -> éªŒè¯ current_device(): 0
   -> éªŒè¯ Tensor ä½ç½®: cuda:0
   -> ç¡¬ä»¶å‹å·: NVIDIA A800-SXM4-80GB
========================================


========================================
ğŸ‘‹ å¤§å®¶å¥½! æˆ‘æ˜¯å…¨å±€ Rank: 7 (è¿›ç¨‹ PID: 49320)
ğŸ“ æ‰€åœ¨èŠ‚ç‚¹: HOST-10-140-60-136

ğŸ‘€ ã€æ‰€è§ã€‘ç‰©ç†è§†é‡:
   åœ¨è¿™å°æœºå™¨ä¸Šï¼Œæˆ‘ç‰©ç†ä¸Šèƒ½çœ‹åˆ° 2 å¼  GPUã€‚

ğŸª‘ ã€æ‰€å¾—ã€‘æŠ¢æ¤…å­ç»“æœ:
âœ… å·²ç»‘å®šé€»è¾‘åº§ä½: 1
   -> éªŒè¯ current_device(): 1
   -> éªŒè¯ Tensor ä½ç½®: cuda:1
   -> ç¡¬ä»¶å‹å·: NVIDIA A800-SXM4-80GB
========================================


========================================
ğŸ‘‹ å¤§å®¶å¥½! æˆ‘æ˜¯å…¨å±€ Rank: 3 (è¿›ç¨‹ PID: 10499)
ğŸ“ æ‰€åœ¨èŠ‚ç‚¹: HOST-10-140-60-38

ğŸ‘€ ã€æ‰€è§ã€‘ç‰©ç†è§†é‡:
   åœ¨è¿™å°æœºå™¨ä¸Šï¼Œæˆ‘ç‰©ç†ä¸Šèƒ½çœ‹åˆ° 2 å¼  GPUã€‚

ğŸª‘ ã€æ‰€å¾—ã€‘æŠ¢æ¤…å­ç»“æœ:
âœ… å·²ç»‘å®šé€»è¾‘åº§ä½: 1
   -> éªŒè¯ current_device(): 1
   -> éªŒè¯ Tensor ä½ç½®: cuda:1
   -> ç¡¬ä»¶å‹å·: NVIDIA A800-SXM4-80GB
========================================


========================================
ğŸ‘‹ å¤§å®¶å¥½! æˆ‘æ˜¯å…¨å±€ Rank: 1 (è¿›ç¨‹ PID: 10497)
ğŸ“ æ‰€åœ¨èŠ‚ç‚¹: HOST-10-140-60-38

ğŸ‘€ ã€æ‰€è§ã€‘ç‰©ç†è§†é‡:
   åœ¨è¿™å°æœºå™¨ä¸Šï¼Œæˆ‘ç‰©ç†ä¸Šèƒ½çœ‹åˆ° 2 å¼  GPUã€‚

ğŸª‘ ã€æ‰€å¾—ã€‘æŠ¢æ¤…å­ç»“æœ:
âœ… å·²ç»‘å®šé€»è¾‘åº§ä½: 1
   -> éªŒè¯ current_device(): 1
   -> éªŒè¯ Tensor ä½ç½®: cuda:1
   -> ç¡¬ä»¶å‹å·: NVIDIA A800-SXM4-80GB
========================================


========================================
ğŸ‘‹ å¤§å®¶å¥½! æˆ‘æ˜¯å…¨å±€ Rank: 2 (è¿›ç¨‹ PID: 10498)
ğŸ“ æ‰€åœ¨èŠ‚ç‚¹: HOST-10-140-60-38

ğŸ‘€ ã€æ‰€è§ã€‘ç‰©ç†è§†é‡:
   åœ¨è¿™å°æœºå™¨ä¸Šï¼Œæˆ‘ç‰©ç†ä¸Šèƒ½çœ‹åˆ° 2 å¼  GPUã€‚

ğŸª‘ ã€æ‰€å¾—ã€‘æŠ¢æ¤…å­ç»“æœ:
âœ… å·²ç»‘å®šé€»è¾‘åº§ä½: 0
   -> éªŒè¯ current_device(): 0
   -> éªŒè¯ Tensor ä½ç½®: cuda:0
   -> ç¡¬ä»¶å‹å·: NVIDIA A800-SXM4-80GB
========================================


========================================
ğŸ‘‹ å¤§å®¶å¥½! æˆ‘æ˜¯å…¨å±€ Rank: 0 (è¿›ç¨‹ PID: 10496)
ğŸ“ æ‰€åœ¨èŠ‚ç‚¹: HOST-10-140-60-38

ğŸ‘€ ã€æ‰€è§ã€‘ç‰©ç†è§†é‡:
   åœ¨è¿™å°æœºå™¨ä¸Šï¼Œæˆ‘ç‰©ç†ä¸Šèƒ½çœ‹åˆ° 2 å¼  GPUã€‚

ğŸª‘ ã€æ‰€å¾—ã€‘æŠ¢æ¤…å­ç»“æœ:
âœ… å·²ç»‘å®šé€»è¾‘åº§ä½: 0
   -> éªŒè¯ current_device(): 0
   -> éªŒè¯ Tensor ä½ç½®: cuda:0
   -> ç¡¬ä»¶å‹å·: NVIDIA A800-SXM4-80GB
========================================
```


# 3. å¦‚ä½•ä¸ºDistributed Trainingä¸åŒçš„å¯åŠ¨æ–¹å¼åˆ†é…GPUä¸task 

| ç‰¹æ€§                | Slurm åŸç”Ÿæ–¹å¼ (æ‚¨çš„ç†è§£)                | Torchrun æ–¹å¼ (ç°åœ¨ä¹Ÿå¾ˆæµè¡Œ)                    |
|:------------------ |:--------------------------------------- |:---------------------------------------------- |
| å‘½ä»¤                | python train.py                          | torchrun ... train.py                          |
| srun å‚æ•°           | --ntasks-per-node=4                      | --ntasks-per-node=1                            |
| è°è´Ÿè´£ç”Ÿå­©å­?       | Slurm ç›´æ¥ç”Ÿæˆ 4 ä¸ª Python è¿›ç¨‹          | Slurm ç”Ÿæˆ 1 ä¸ª torchrunï¼Œtorchrun å†ç”Ÿ 4 ä¸ª Python |
| ç¯å¢ƒé…ç½®            | éš¾ï¼šå¿…é¡»æ‰‹åŠ¨å†™ä»£ç è·å– Rank å’Œ Master IP | æ˜“ï¼štorchrun è‡ªåŠ¨è®¾ç½®å¥½ç¯å¢ƒå˜é‡                |
| ä»£ç ä¾èµ–            | ä»£ç æ·±åº¦ç»‘å®š Slurm å˜é‡ (SLURM_PROCID)   | ä»£ç æ›´é€šç”¨ï¼Œä¸ç”¨å†™æ­» Slurm å˜é‡                |
| é€‚ç”¨åœºæ™¯            | é«˜æ€§èƒ½è®¡ç®—ä¸­å¿ƒ (HPC) æ ‡å‡†åšæ³•            | äº‘æœåŠ¡å™¨ / ä¸ªäººå•æœºå¸¸ç”¨åšæ³•                    |

## 3.1 Slurm åŸç”Ÿå¯åŠ¨æ–¹å¼

```bash
srun -p evla2_t --gres=gpu:4 --ntasks-per-node=4 python train.py
```
## 3.2 Torchrun æ–¹å¼
åœ¨ç½‘ä¸Šä¼šçœ‹åˆ°å¦ä¸€ç§å†™æ³•ï¼Œè¿™ç§å†™æ³•çœ‹èµ·æ¥å¾ˆå¥‡æ€ªï¼š åªç”³è¯· 1 ä¸ªä»»åŠ¡ï¼Œå´è·‘ 4 ä¸ª GPUã€‚
```bash
# å¦ä¸€ç§æµæ´¾ (Torchrun æ–¹å¼)
srun -p evla2_t --gres=gpu:4 --ntasks-per-node=1 --cpus-per-task=16 \
    torchrun --nproc_per_node=4 train.py
```


